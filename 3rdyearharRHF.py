# -*- coding: utf-8 -*-
"""3rdYearHAR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PFTmKTMhj1lRhzKGSNkppmx8VNuL1LoO

# Imports, Data Handling
"""

import numpy as np
import pandas as pd
import os
import scipy as sp
from joblib import Parallel, delayed
import random
import statsmodels.api as sm
import sys
import copy
import numpy.matlib
import math
import matplotlib.pyplot as plt


#Load Data, uncomment when you need to load a dataset. This chunk is for the realized volatility dataset
temp=pd.read_csv('returns.csv')
data=temp.to_numpy()
"""# Models"""


def dataEndGen(data,target):
    dataEnd=np.roll(np.log(np.array(rvs)),-1*target)
    dataEnd[-1*target]=np.nan
    return dataEnd

def dataExgGen(data,rvs,lags,skipper):
    dataExg=np.empty(shape=(rvs.shape[0],len(lags)))
    for j in range(len(lags)):
        temp=np.zeros(shape=(data.shape[0]))
        if j>0:
            for k in range(lags[j]-lags[j-1]):
                temp=temp+np.roll(np.log(np.array(data[:])),(lags[j]-k+1))
            temp=temp/(lags[j]-lags[j-1])
        else:
            for k in range(lags[j]-0):
                temp=temp+np.roll(np.log(np.array(data[:])),(lags[j]-k+1))
            temp=temp/(lags[j]-0)
        temp[0:lags[j]]=np.nan
        dataExg[:,j]=temp[range(0,data.shape[0],skipper)] #select only valid choices
    dataExg = sm.add_constant(dataExg) #dataset created
    return dataExg

#HAR Stuff
def HARmse(rvs,data,target,lags,window,skipper,valid):
    dataEnd=dataEndGen(rvs,target)
    dataExg=dataExgGen(data,rvs,lags,skipper)
    maxLags=lags[-1]//skipper
    preds=np.empty(shape=(valid,))
    testing=np.empty(shape=(valid,))
    for j in range(valid):
        try: #Figure out how to do this better?
            model=sm.OLS(dataEnd[j+maxLags:window+j+maxLags], dataExg[j+maxLags:window+j+maxLags,:], missing='drop',hasconst=True)
            modelResults=model.fit()
            preds[j]=modelResults.predict(dataExg[window+j+maxLags,:])[0]
            testing[j]=dataEnd[window+j+maxLags]
        except ValueError:  #raised if `y` is empty.
            preds[j]=np.nan
            testing[j]=dataEnd[window+j+maxLags]
    return MSE(testing, preds)

def TreeModelingR(rvs,data,depth,MaxLags,loc,window,val,target,skipper):
    prev=(MaxLags,)
    dailylags=MaxLags//skipper
    bestmse=HARmse(rvs[loc-dailylags:window+loc+val+1+target],data[skipper*(loc-dailylags):skipper*(window+loc+val+1+target)],target,prev,window,skipper,val)
    bestlags=prev
    update=prev
    for i in range(depth):
        depthmse=100000 #should be larger than any mse by like 8 orders of magnitude
        valid=list()
        for j in range(1,skipper*dailylags):
       #Somehow recursively build tree?
            if j in prev:
                continue
            else:
                testlags=(*prev,j,)
                testlags=tuple(sorted(testlags))
                valid.append(testlags)
        mses=Parallel(n_jobs=1,return_as='list')(delayed(HARmse) (rvs[loc-dailylags:window+loc+val+target+1],data[skipper*(loc-dailylags):skipper*(window+loc+val+1+target)],target,valid[j],window,skipper,val) for j in range(len(valid))) #single-thread due to lack of threads problem
        for j in range(len(mses)):
            if mses[j]<bestmse:
                bestmse=mses[j]
                bestlags=valid[j]
            if mses[j]<depthmse:
                depthmse=mses[j]
                update=valid[j]
        prev=update
    return bestlags

#DM stuff
def HARgen(rvs,data,target,lags,window,valid,test,skipper):
    dataEnd=dataEndGen(rvs,target)
    dataExg=dataExgGen(data,rvs,lags,skipper)
    maxLags=lags[-1]//skipper
    preds=np.empty(shape=(testsize,))
    testing=np.empty(shape=(testsize,))
    for j in range(test):
        try: #Figure out how to do this better?
            model=sm.OLS(dataEnd[j+valid+maxLags:window+j+valid+maxLags], dataExg[j+valid+maxLags:window+j+valid+maxLags,:], missing='drop',hasconst=True)
            modelResults=model.fit()
            preds[j]=modelResults.predict(dataExg[window+j+val+maxLags,:])[0]
            testing[j]=dataEnd[window+j+val+maxLags]
        except ValueError:  #raised if `y` is empty.
            preds[j]=np.nan
            testing[j]=dataEnd[window+j+val+maxLags]
    return testing, preds

def predsgen(rvs,data,depth,MaxLags,loc,window,val,testsize,target,skipper):
    tracker=0
    pick=TreeModelingR(rvs,data,depth,MaxLags,loc,window,val,target,skipper)
    if len(pick)==depth:
        tracker=tracker+1
    dailylags=MaxLags//skipper
    actual,preds=HARgen(rvs[loc-dailylags:window+val+loc+target+testsize+1],data[skipper*(loc-dailylags):skipper*(window+loc+val+testsize+1+target)],target,pick,window,val,testsize,skipper)
    return actual,preds,tracker

def lossgen(actual, p1, p2):
    # Initialise lists
    e1_lst = np.empty(shape=actual.shape)
    e2_lst = np.empty(shape=actual.shape)
    d_lst  = np.empty(shape=actual.shape)
    # construct d according to crit
    e1=np.square(actual - p1)
    e2=np.square(actual - p2)
    d_lst=e1-e2
    return d_lst

def dm_test(d_lst,h=1):
    # Mean of d
    mean_d = np.nanmean(d_lst)
    T = float(d_lst.size - np.isnan(d_lst).sum())
    # Find autocovariance and construct DM test statistics
    def autocovariance(Xi, N, k, Xs):
        autoCov = 0
        T = 0
        for i in np.arange(0, N-k):
              if np.isnan((Xi[i+k]-Xs)*(Xi[i]-Xs)):
                  continue
              else:
                  autoCov += (Xi[i+k]-Xs)*(Xi[i]-Xs)
                  T=T+1
        return (1/(T))*autoCov
    gamma = []
    for lag in range(0,h):
        gamma.append(autocovariance(d_lst,len(d_lst),lag,mean_d)) # 0, 1, 2
    V_d=0
    if gamma[0]==np.nan:
        V_d = 2*np.nansum(gamma[1:])/T
    else:
        V_d = (gamma[0]+ 2*np.nansum(gamma[1:]))/T
    DM_stat=(np.abs(V_d)**(-0.5))*mean_d
    harvey_adj=((T+1-2*h+h*(h-1)/T)/T)**(0.5)
    DM_stat = harvey_adj*DM_stat
    return  DM_stat


"""# Utility Functions"""

#utility functions

def MSE(x,y):
    return np.sqrt(np.nanmean(np.square(x - y)))

"""# Configs

"""

MaxDepth=3 #restricted for noise/computation purposes
val=500
testsize=1
window=500
targets=(1,5,22,44,66)
Finals=np.empty(shape=(len(targets),6))
times=["09:30","09:35","09:40","09:45","09:50","09:55"]
for i in range(10,16):
    for j in range(0,60,5):
        if j<10:
            times.append(str(i)+":0"+str(j))
        else:
            times.append(str(i)+":"+str(j))
times.append("16:00")
MaxLags=22*len(times)
dailylags=22
data=data[~np.any(data==0,axis=1)]
rvs=np.sqrt(data[:,-1])
data=np.sqrt(data[:,:-1])
data=np.reshape(data, (data.shape[0]*data.shape[1],))
for i in range(len(targets)):
    actual,preds,tracker=zip(*Parallel(n_jobs=-1, return_as='list')(delayed (predsgen)(rvs,data,MaxDepth,MaxLags,j,window,val,testsize,targets[i],len(times)) for j in range(dailylags,rvs.shape[0]-window-val-testsize,testsize)))
    print(sum(tracker))
    actual=np.array(actual)
    preds=np.array(preds)
    _,hfhars=zip(*Parallel(n_jobs=-1,return_as='list') (delayed (HARgen) (rvs[j-dailylags:window+j+val+testsize+1+targets[i]],data[j*len(times)-MaxLags:len(times)*(window+j+val+testsize+1+targets[i])],targets[i],(1*len(times),5*len(times),22*len(times)),window,val,testsize,len(times)) for j in range(dailylags,rvs.shape[0]-window-val-testsize,testsize)))
    hfhars=np.array(hfhars)
    _,hars=zip(*Parallel(n_jobs=-1,return_as='list') (delayed (HARgen) (rvs[j-dailylags:window+j+val+testsize+1+targets[i]],rvs[j-dailylags:window+j+val+testsize+1+targets[i]],targets[i],(1,5,22),window,val,testsize,1) for j in range(dailylags,rvs.shape[0]-window-val-testsize,testsize)))
    hars=np.array(hars)
    actual=np.reshape(actual,actual.shape[0]*actual.shape[1],)
    preds=np.reshape(preds,preds.shape[0]*preds.shape[1],)
    hars=np.reshape(hars,hars.shape[0]*hars.shape[1],)
    hfhars=np.reshape(hfhars,hfhars.shape[0]*hfhars.shape[1])
    losses=lossgen(actual,hars,preds)
    print(np.count_nonzero(~np.isnan(actual)))
    print(np.count_nonzero(~np.isnan(preds)))
    print(np.count_nonzero(~np.isnan(hars)))
    print(np.count_nonzero(~np.isnan(hfhars)))
    print(np.count_nonzero(~np.isnan(losses)))
    losses2=lossgen(actual,hfhars,preds)
    Finals[i,0]=MSE(actual,preds)/MSE(actual,hars)
    Finals[i,1]=dm_test(losses,h=targets[i])
    Finals[i,2]=dm_test(losses2,h=targets[i])
#DAILY stuff here.
MaxDepth=3 #reset for runtime
MaxLags=22
for i in range(len(targets)):
    actuald,predsd,tracker=zip(*Parallel(n_jobs=-1, return_as='list')(delayed (predsgen)(rvs,rvs,MaxDepth,MaxLags,j,window,val,testsize,targets[i],1) for j in range(MaxLags,rvs.shape[0]-window-val-testsize,testsize)))
    print(sum(tracker))
    actuald=np.array(actuald)
    predsd=np.array(predsd)
    actuald=np.reshape(actuald,actuald.shape[0]*actuald.shape[1],)
    predsd=np.reshape(predsd,predsd.shape[0]*predsd.shape[1],)
    losses=lossgen(actual,predsd,hars)
    losses2=lossgen(actual,predsd,hfhars)
    Finals[i,3]=MSE(actuald,predsd)/MSE(actuald,hars)
    Finals[i,4]=dm_test(losses,h=targets[i])
    Finals[i,5]=dm_test(losses2,h=targets[i])
s4="RHF/Finals.txt"
np.savetxt(s4,Finals,fmt="%.4f", delimiter= " & ")
